
= Mandrel
Vincent Devillers <https://twitter.com/treydone[@Treydone]>;

[NOTE]
.This document is under active development
====
If you find errors or omissions in this document, please don't hesitate to submit an issue or open a pull request with a fix.
New contributors are always welcome!
====

Mandrel is a distributed mining engine built for the cloud. Features include:

* Distributed and Highly Available mining Engine.
* Powerful and customizable
* HTTP RESTful API
* Open Source under the Apache License, version 2 ("ALv2")

= Introduction

== What is Mandrel?

Mandrel is designed as a whole product, with the idea of bringing a complete mining engine solution that runs out of the box.

=== The Big Picture
Mandrel is designed for performance and scalability. Event if it has been tested on small and on medium-sized cluster (~30 nodes), Mandrel can scale on cloud-sized cluster without any effort.

Mandrel is NOT:

- a search engine (but you can use Elasticsearch or Solr -or whatever you want- as output)
- a NoSQL database
- a SDK

=== What for?
Mandrel can be used to build various mining jobs:

- Web crawlers: make the web your in-house database by turning web pages into data
- SEO analytics: test web pages and links for valid syntax and structure, find broken links and 404 pages
- Monitor sites to see when their structure or contents change
- Build a special-purpose search index, intranet/extranet indexation...
- Maintain mirror sites for popular Web sites
- Search for copyright infringements: find duplicate of content

=== Compared to...

==== Nutch
Big, big Java project. Open-source. Built for crawling all the web and can be distributed on several machines, such as on AWS or any other cloud provider, as well as  crawling a specific targeted group of websites, every x days or hours.
The architecture is pretty heavy and has lots of dependencies (ant, gora, hbase, hadoop, etc.). Very powerful and manages scheduling, domain restriction, politeness, etc.
Can be run in standalone without Hadoop. Very polite and makes sure that there is only one query per host running at the same time, to avoid being blacklisted.
Run from the command.

In distributed mode, Nutch need a full Hadoop stack (...) and use (veryyyy) long-running MapReduce in order to sort and crawl URL.

Last version is Nutch 2.x, which is a huge rewrite almost from scratch and therefore not so close to 1.x. However, Nutch 2.x is slower and has less features than Nutch 1.x.

In addition to the crawler feature, Nutch is also a search engine and use Luc√®ne to index documents.

==== Heristrix
Heritrix is the Internet Archive's open-source, extensible, web-scale, archival-quality web crawler project.
Have Web Control management interface. A powerful job definition, but based on Spring beans definition. Not designed to be scalable.

Last version is 3.2.0 (Jan 2014).

==== Scrapy
Open-source Python project, best suited for scraping focused websites. Light and easy to use,.
Useful when building a handmade parser on a known website in order to extract precise informations.

Not distributed by default, so not a right tool for a huge amount of websites or big websites.
Some initiatives aimed to add cluster features (Distributed Frontera and Scrapy Cluster) to Scrapy but are difficult to deploy since they are based on Kafka and/or HBase and need an Hadoop cluster.
Politness is respected, but only one process can download on one host at a time.

=== Internals
Mandrel uses Zookeeper, Thrift, Netty, Undertow and Spring. It can be connected to:

- Mongo
- Elasticsearch
- Kafka
- Cassandra
- Hbase + HDFS
- "Insert your favorite database here"

= Quick Start

== Getting Started

TIP: A useful tip


=== System Requirements
Mandrel works on Linux, Mac and Windows. All you need is Java 8+ and a running instance of Mongo 3.0+.


= Setup

== Installation

=== With the archive

This is the easiest method, you can download the latest version here:
https://dl.bintray.com/treydone/generic/

Just unzip the archive and you are done.

=== With RPMs

==== Using +yum+

 .  Copy this text into a 'mandrel.repo' file on your Linux machine:

[source]
#For Mandrel
[mandrel]
name=mandrel
baseurl=https://dl.bintray.com/treydone/rpm
gpgcheck=0
enabled=1

OR

 . Run the following to get a generated .repo file:

 $ wget https://bintray.com/treydone/rpm/rpm -O mandrel.repo

.  Move the repo file to /etc/yum.repos.d/

 $ sudo mv  mandrel.repo /etc/yum.repos.d/

 . Run the installation command

   RHEL and Fedora 21 or earlier::
 +
  $ sudo yum install mandrel

   Fedora 22 or later::
 +
  $ sudo dnf install mandrel

==== By downloading

You can directly download the rpm by using:

 $ curl -L "https://dl.bintray.com/treydone/rpm/mandrel-XXX.noarch.rpm" -o mandrel.noarch.rpm

 $ rpm -Ivh mandrel.noarch.rpm

=== With DEBs

==== Using +apt-get+

To install Mandrel on Debian Sid or Ubuntu Saucy or greater:

. Using the command line, add the following to your /etc/apt/sources.list system config file:

 $ echo "deb https://dl.bintray.com/treydone/deb {distribution} {components}" | sudo tee -a /etc/apt/sources.list

OR

. Add the repository URLs using the "Software Sources" admin UI:

 deb https://dl.bintray.com/treydone/deb {distribution} {components}

. In a terminal, type the +apt-get+ command

 $ sudo apt-get install mandrel

==== By downloading

You can directly download the deb by using:

 $ curl -L "https://dl.bintray.com/treydone/deb/mandrel-XXX.deb" -o mandrel.deb
 $ dpkg -i mandrel.deb

=== From the sources

Mandrel uses Maven for its build system. Simply run:

[source]
mvn clean install -DskipTests
cd standalone
mvn spring-boot:run -DskipTests

=== With Docker

Coming soon...

=== On Windows

Coming soon...

== Build a release

A release can be built with the maven-release-plugin and pushing the new tag. Travis-CI will then deploy the new tag on Bintray
[source]
mvn release:clean
mvn release:prepare -Darguments="-DskipTests" -DpushChanges=false
git push --follow-tags

If something weird happen, just rollback
[source]
mvn release:rollback
mvn release:clean

Travis-CI: https://travis-ci.org/Treydone/mandrel/

Bintray: https://bintray.com/treydone/maven/mandrel/view

== Configuration

=== Common

==== Discovery

[source]
discovery:
  instanceHost: localhost
  zookeeper:
    enabled: true
    connectString: localhost:2181
    root: /mandrel

[[discovery.instanceHost]]
*`discovery.instanceHost`*::
+
.Description
The address what will be registered in the discovery.
+
.Default
`localhost`

[[discovery.zookeeper.connectString]]
*`discovery.zookeeper.connectString`*::
+
.Description
Comma separated list of servers in the ZooKeeper ensemble.
    For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
    By default this is set to localhost for local and pseudo-distributed modes
    of operation. For a fully-distributed setup, this should be set to a full
    list of ZooKeeper ensemble servers.
+
.Default
`localhost:2181`

[[discovery.zookeeper.root]]
*`discovery.zookeeper.root`*::
+
.Description
The root path in Zookeeper where the services will be registered.
+
.Default
`/mandrel`

==== Transport

[source]
transport:
  bindAddress: localhost
  port: 8090

==== Logging

[source]
logging:
  console:
    enabled: true
    level: WARN
  level:
    org.springframework: INFO
    io.mandrel: DEBUG
    io.mandrel.messaging: DEBUG

=== controller.yml

[source]
server:
  port: 8080
  undertow:
    buffer-size: 16000
    buffers-per-region: 20
    direct-buffers: true
    io-threads: 4
    worker-threads: 32


[[server.port]]
*`server.port`*::
+
.Description
The port used for all HTTP incoming traffic.
+
.Default
`8080`

=== frontier.yml

=== worker.yml

=== standalone.yml

[source]
spring:
  pidfile: standalone.pid
  application:
    name: standalone
    admin:
      enabled: false
  data:
    mongodb:
      uri: mongodb://localhost:27017/mandrel
  jmx:
    enabled: false
discovery:
  local:
    enabled: true
  zookeeper:
    enabled: false

== Running modes

=== Standalone - Single-JVM

[source]
discovery:
  local:
    enabled: true
  zookeeper:
    enabled: false

 $ mandrel-standalone start

 $ mandrel-standalone stop

=== Standalone - Pseudo-distributed

[source]
discovery:
  zookeeper:
    enabled: true

 $ mandrel-standalone start

 $ mandrel-standalone stop

=== Distributed

[source]
$ mandrel-controller start
$ mandrel-frontier start
$ mandrel-worker start

[source]
$ mandrel-worker start
$ mandrel-frontier start
$ mandrel-controller stop


= Architecture

== Overview

== Components

=== Controller

=== Frontier

The goal of the frontier is to know which URI to process next, and when.
The frontier decides the logic and policies to follow when a crawler is visiting sources like websites: what pages should be crawled next, priorities and ordering, how often pages are revisited, etc.
It keeps the state of the crawl. This includes, but is not limited to:

- What URIs have been discovered
- What URIs are being processed (fetched)
- What URIs have been processed

The frontier garanties the respect of the politeness like the bandwidth limits or the number of pages to be crawled.

The frontier is set of various background tasks:

- Priorizer: from a set of URIs, schedule the priority of the URIs and push them in the internal queues
- Revister: revist a page, when and how

==== Revisit policies

- Freshness: This is a binary measure that indicates whether the local copy is accurate or not.
- Age: This is a measure that indicates how outdated the local copy is.
- Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change.
- Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency.

==== Politeness

- Parallel connections
- Max pages per second
- Max bytes per second/bandwidth

=== Worker

The goal of the worker is simple, it download and parse the content of uri given by the frontiers. Its workflow is more or less the following:

- Pick out a uri from the frontier
- Fetch the content
- Store raw results in blobstore
- Find links in the content
- Store metadata in metadatastore
- If extraction needed, parse the content and store the results in documentstore

== Scalabilty

All the components in Mandrel can have multiple instances:

- Multiple controllers for high-availibility on the main component in Mandrel deployment
- Multiple  frontiers in order to dsitribute the heavy job of priorization and reschedule
- Multiple  workers in order to grow up the bandwidth

All these instances are registered in a discovery service. By default, Mandrel uses Zookeeper as a discovery service.
An other discovery service, 'local', is present but only used by the standalone mode.
Other discovery services caneasily be added by implementing these interfaces:

[source]
import java.util.List;
public interface DiscoveryClient {
	ServiceInstance register(ServiceInstance instance);
	void unregister(String serviceId);
	List<ServiceInstance> getInstances(String serviceId);
	List<String> getServices();
	String getInstanceHost();
	ServiceInstance getLocalInstance(String serviceId);
	ServiceInstance getInstance(String id, String serviceId);
	String getInstanceId();
}

Example:

[source]
@ConditionalOnProperty(value = "discovery.mynewdiscovery.enabled", matchIfMissing = false)
@Component
public class MyNewDiscoveryClient implements DiscoveryClient {
        ...
}

And finally by enabling the new discovery service in the properties:

[source]
discovery:
  local:
    enabled: false
  zookeeper:
    enabled: false
  mynewdiscovery:
    enabled: true

= Using Mandrel

== Web UI

== API REST

All the documentation can be found on the Swagger endpoint at:  //TODO

=== API Conventions

=== Endpoints

==== Spiders

[[spiders]]
*GET `/spiders`*::
+
.Description
List all the spider

[[spiders_id]]
*GET `/spiders/{id}`*::
+
.Description
Return a spider

[[spiders_start]]
*GET `/spiders/{id}/start`*::
+
.Description
Start

[[spiders_stop]]
*GET `/spiders/{id}/stop`*::
+
.Description
Stop


==== Nodes

[[nodes]]
*GET `/nodes`*::
+
.Description
List all the nodes

[[node]]
*GET `/nodes/{id}`*::
+
.Description
Find a node by its id

==== Data

==== Cluster

== Spider definition

=== Introduction

A spider is at least composed by:

- sources: a set of sources (static list of uris, files, endpoint...) containing uris
- stores: where to stores the raw data and their metadata
- frontier: the list of uris discovered to be fetched or revisited
- client: the bridge between Mandrel and the uris to be crawled, by default contains an HTTP/S and a FTP/S client

You can also define:

- filters: if you want to fetch only a specific type of uri (on the same domain, only starting with a prefix...)
- extractors: if your want to extract some data from the downloaded content

=== Examples

Let's see some examples!

==== IMDB

==== LinkedIn

=== Filtering

You can add filters to your spider. There are two types of filters:

* link filters
* blob filters

Link filters apply conditions only on the link whereas blob filters apply conditions on the downloaded blob. This means also that link filters are applied BEFORE the crawling and blob filters AFTER. Take this in consideration when developping new spiders

==== Link filters

*Domains*

Example:
[source]
{
    "type":"allowed_for_domains",
    "domains": [
        "mydomain1",
        "mydomain2"
    ]
}

*Skip ancor*

Example:

[source]
{
    "type":"skip_ancor"
}

*Pattern*

Example:

[source]
{
    "type":"pattern",
    "pattern": "..."
}

*Sanitize*

Remove all the parameters in a URI (tck=..., timestamp=..., adsclick=...)

Example:

[source]
{
    "type":"sanitize_params"
}

*Booleans*

or|and|not|true|false

Example:

[source]
{
  "not": {
      "type":"allowed_for_domains",
      "domains": [
          "mydomain1",
          "mydomain2"
      ]
  }
}

[source]
{
  "and": [
      {
          "type":"allowed_for_domains",
          "domains": [
              "mydomain1",
              "mydomain2"
          ]
      },
      {
          "type":"pattern",
          "pattern": "..."
      }
  ]
}

To be continued...

* Keep only some parameters
* ...

==== Blob filters

*Size*

*Booleans*

or|and|not|true|false

To be continued...


=== Raw exporting

Your spider is now done. Or not. We don't care, we just want to export the raw data of the pages/documents. You have to two ways to do this:

- Extract the data from the page store if you have specified one during the creation (SQL, Cassandra...)
- Use the dedicated endpoint

[source]
$ curl -X GET http://localhost:8080/spiders/wikipedia/raw/export?format=csv|json

To be continued...

- Define options for the exporters
- Add formats for parquet
- Support compression

=== Extracting

Somethimes we want to crawl pages. But what we really want is the data INSIDE the pages.

[source]
$ curl -X POST http://localhost:8080/spiders/imdb -d '
{
   "sources":[
      {
         "type":"fixed",
         "urls":[
            "http://www.imdb.com/"
         ]
      }
   ],
   "extractors":[
      {
         "name":"movie_extractor"
         "filters":[
            {
               "type":"patterns",
               "value":[
                  "/title"
               ]
            }
         ],
         "fields":[
            {
               "title":{
                  "extractor":{
                     "type":"xpath",
                     "value":"//*[@id="overview-top"]/h1/span[1]/text()",
                     "source":"body"
                  }
               }
            },
            {
               "description":{
                  "extractor":{
                     "type":"xpath",
                     "value":"//*[@id="overview-top"]/p[2]/text()",
                     "source":"body"
                  }
               }
            },
            {
               "actors":{
                  "extractor":{
                     "type":"xpath",
                     "value":"//*[@id="overview-top"]/div[6]/a/span",
                     "source":"body"
                  }
               }
            }
         ]
      }
   ]
}
'

This will extract the fields 'title', 'description' and 'actors' from the page.

=== Data exporting

Ok, now we got some data, we can export them by calling:

[source]
$ curl -X POST http://localhost:8080/spiders/export/movie_extractor?format=csv|json

==== JSON

[source]
{
    "type":"json"
}

==== Delimited separated values

[source]
{
    "type":"csv",
    "quote_char":"\"",
    "delimiter_values":44,
    "delimiter_multivalues":124,
    "keep_only_first_value":false,
    "add_header":true,
    "end_of_line_symbols":"\r\n"
}

=== Blob & metadata stores

Example for using Mongo:

[source]
{
   "stores":{
      "metadata":{
         "type":"mongo"
      },
      "blob":{
         "type":"mongo"
      }
   }
}

The store for blob is not mandatory, if you extract data via extractors for instance, but the metadata is:

[source]
{
   "stores":{
      "metadata":{
         "type":"mongo"
      },
      "blob":null
   }
}

==== Mongo

Blob, metadata, document

[source]
"stores" : {
        "metadata" : {
                "type" : "mongo",
                "uri" : "mongodb://localhost",
                "database" : "mandrel",
                "collection" : "metadata_{0}",
                "batch_size" : 1000
        },
        "blob" : {
                "type" : "mongo",
                "uri" : "mongodb://localhost",
                "database" : "mandrel",
                "bucket" : "blob_{0}",
                "batch_size" : 10
        }
}

=== Document stores

==== Mongo

[source]
"extractors" : {
        "data" : [
                {
                        "store" : {
                                "type" : "mongo",
                                "uri" : "mongodb://localhost",
                                "database" : "mandrel",
                                "collection" : "document_{0}",
                                "batch_size" : 1000
                        }
                }
        ]
}

==== Elasticsearch

Document

[source]
"extractors" : {
        "data" : [
                {
                        "store" : {
                                "type" : "elasticsearch",
                                "addresses" : ["localhost:9300"],
                                "type" : "document",
                                "index" : "mandrel_{0}",
                                "cluster" : "mandrel",
                                "batch_size" : 1000
                        }
                }
        ]
}


==== Redis

==== Mutliple output

=== Client

Each spider can be configured with a specified client in order to configure:

* Proxies
* Request and connection timeouts
* User-agent generation
* Custom cookies (jsessionid...) and headers (X-Request-By, Basic-Authentication...)
* DNS resolution strategies
...

[source]
{
 "request_time_out":3000,
 "headers":null,
 "params":null,
 "follow_redirects":false,
 "cookies":null,
 "user_agent_provisionner":{
     "type":"fixed",
     "ua":"Mandrel"
 },
 "dns_cache":{
     "type":"internal"
 },
 "proxy":{
     "type":"no"
 },
 "politeness":{
     "global_rate":1000,
     "per_node_rate":500,
     "max_pages":500,
     "wait":100,
     "ignore_robots_txt":false,
     "recrawl_after":-1
 }
}


== Security

== Production considerations & performance tuning

NOTE: Section pending

By default, Mandrel is started in a standalone mode. In this mode, the 3 main components are started in the same JVM. Altought this may be useful for testing purposes, the standalone mode does not allow you to scale your deployment.

For production, we recommand you to deploy at least one controller, one frontier and one worker, each in separate JVM on a dedicated serveur.

=== General Guidelines

=== Operating System

==== 64-bit
Use a 64-bit platform (and 64-bit JVM).

==== Swapping
Watch out for swapping. Set swappiness to 0.

=== Network

=== Java

=== ZooKeeper

=== Case studies

== Troubleshooting

NOTE: Section pending

== Glossary

NOTE: Section pending

== Copyright and License

NOTE: Section pending

This software is licensed under the Apache License, version 2 ("ALv2"), quoted below.

Copyright 2009-2016 Mandrel

Licensed under the Apache License, Version 2.0 (the "License"); you may not
use this file except in compliance with the License. You may obtain a copy of
the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations under
the License.
